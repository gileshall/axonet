{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/gileshall/axonet/blob/main/notebooks/axonet_training_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Axonet End-to-End Training Tutorial\n",
    "\n",
    "This notebook walks through the full axonet training pipeline:\n",
    "\n",
    "1. **Install** axonet and dependencies\n",
    "2. **Download neurons** from NeuroMorpho.Org\n",
    "3. **Generate dataset** \u2014 render multi-view images with segmentation masks and depth maps\n",
    "4. **Train VAE (Stage 1)** \u2014 train a SegVAE2D with variational skip connections\n",
    "5. **Fine-tune CLIP (Stage 2)** \u2014 contrastive learning to align neuron images with text descriptions\n",
    "6. **Evaluate CLIP** \u2014 retrieval metrics, zero-shot classification, and t-SNE visualization\n",
    "\n",
    "**Runtime requirement:** Set your Colab runtime to **GPU** (T4 recommended):\n",
    "`Runtime > Change runtime type > T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "All configurable variables are defined here. The defaults use a small demo scale (~50 neurons). Increase `N_NEURONS` and epoch counts for real training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ---- Scale ----\n",
    "N_NEURONS = 50            # Number of neurons to download (demo scale)\n",
    "N_VIEWS = 24              # Views per neuron (matches PCA default: 6 canonical + 12 biased + 6 random)\n",
    "VAL_RATIO = 0.15          # Fraction held out for validation\n",
    "IMAGE_SIZE = 512          # Rendered image resolution\n",
    "\n",
    "# ---- VAE (Stage 1) ----\n",
    "BATCH_SIZE_VAE = 8\n",
    "MAX_EPOCHS_VAE = 10\n",
    "LR_VAE = 1e-4\n",
    "KLD_WEIGHT = 0.1\n",
    "\n",
    "# ---- CLIP (Stage 2) ----\n",
    "BATCH_SIZE_CLIP = 64\n",
    "MAX_EPOCHS_CLIP = 10\n",
    "LR_CLIP = 1e-4\n",
    "CLIP_EMBED_DIM = 512\n",
    "TEXT_ENCODER = \"distilbert-base-uncased\"\n",
    "TEMPERATURE = 0.07\n",
    "\n",
    "# ---- Paths ----\n",
    "WORK_DIR       = \"/content/axonet_tutorial\"\n",
    "NEURON_DIR     = f\"{WORK_DIR}/neurons\"\n",
    "DATASET_DIR    = f\"{WORK_DIR}/dataset\"\n",
    "STAGE1_CKPT_DIR = f\"{WORK_DIR}/checkpoints/stage1\"\n",
    "STAGE2_CKPT_DIR = f\"{WORK_DIR}/checkpoints/clip\"\n",
    "LOG_DIR        = f\"{WORK_DIR}/logs\"\n",
    "EVAL_DIR       = f\"{WORK_DIR}/eval_results\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install\n",
    "\n",
    "Install system libraries for headless OpenGL rendering (EGL) and the axonet package with CLIP dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# System deps for headless OpenGL (moderngl EGL backend)\n",
    "!apt-get -qq install libegl1-mesa-dev libgles2-mesa-dev > /dev/null 2>&1\n",
    "\n",
    "# Install axonet with CLIP extras\n",
    "!pip install -q \"axonet[clip] @ https://github.com/gileshall/axonet/archive/refs/heads/main.zip\"\n",
    "\n",
    "# Verify import\n",
    "import axonet\n",
    "print(f\"axonet imported successfully\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be very slow.\")\n",
    "    print(\"Go to Runtime > Change runtime type > T4 GPU\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Neurons\n",
    "\n",
    "Download SWC morphology files from [NeuroMorpho.Org](https://neuromorpho.org). We query for mouse neurons and fetch both the standardized SWC files and morphometry measurements."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!python -m axonet.utils.neuromorpho_bulk \\\n",
    "    --query 'species:mouse' \\\n",
    "    --out {NEURON_DIR} \\\n",
    "    --max-pages 1 \\\n",
    "    --page-size {N_NEURONS} \\\n",
    "    --find \\\n",
    "    --fetch-morphometry \\\n",
    "    --insecure"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "swc_dir = Path(NEURON_DIR) / \"swc\"\n",
    "swc_files = sorted(swc_dir.glob(\"*.swc\")) + sorted(swc_dir.glob(\"*.SWC\"))\n",
    "print(f\"Downloaded {len(swc_files)} SWC files\")\n",
    "\n",
    "# Parse metadata for distribution info\n",
    "metadata_path = Path(NEURON_DIR) / \"metadata.jsonl\"\n",
    "cell_types, regions = [], []\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path) as f:\n",
    "        for line in f:\n",
    "            m = json.loads(line)\n",
    "            cell_types.append(m.get(\"cell_type\", \"unknown\"))\n",
    "            regions.append(m.get(\"brain_region\", [\"unknown\"])[0] if isinstance(m.get(\"brain_region\"), list) else m.get(\"brain_region\", \"unknown\"))\n",
    "\n",
    "    print(f\"\\nCell type distribution (top 10):\")\n",
    "    for ct, n in Counter(cell_types).most_common(10):\n",
    "        print(f\"  {ct}: {n}\")\n",
    "\n",
    "    print(f\"\\nBrain region distribution (top 10):\")\n",
    "    for br, n in Counter(regions).most_common(10):\n",
    "        print(f\"  {br}: {n}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Preview first SWC file\n",
    "if swc_files:\n",
    "    sample = swc_files[0]\n",
    "    print(f\"Sample: {sample.name}\")\n",
    "    print(\"-\" * 60)\n",
    "    with open(sample) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 15:\n",
    "                print(\"...\")\n",
    "                break\n",
    "            print(line.rstrip())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Dataset\n",
    "\n",
    "Render multi-view images of each neuron using PCA-guided camera placement:\n",
    "- **6 canonical views** along principal component axes\n",
    "- **12 biased views** concentrated near the PC1-PC2 plane (largest projected area)\n",
    "- **6 random views** for diversity\n",
    "\n",
    "Each view produces four outputs:\n",
    "- `mask_bw` \u2014 binary silhouette (VAE input)\n",
    "- `mask` \u2014 class-ID segmentation map (VAE target)\n",
    "- `mask_color` \u2014 colorized segmentation (visualization)\n",
    "- `depth` \u2014 depth map (VAE target)\n",
    "\n",
    "Data is split into train/val at the neuron level."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!python -m axonet.training.dataset_generator \\\n",
    "    --swc-dir {NEURON_DIR}/swc \\\n",
    "    --out {DATASET_DIR} \\\n",
    "    --views {N_VIEWS} \\\n",
    "    --sampling pca \\\n",
    "    --adaptive-framing \\\n",
    "    --width {IMAGE_SIZE} \\\n",
    "    --height {IMAGE_SIZE} \\\n",
    "    --val-ratio {VAL_RATIO} \\\n",
    "    --margin 0.40 \\\n",
    "    --supersample-factor 2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def count_manifest(path):\n",
    "    \"\"\"Count samples and unique neurons in a manifest.\"\"\"\n",
    "    ids = set()\n",
    "    n = 0\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "            n += 1\n",
    "            ids.add(entry.get(\"swc\", \"\"))\n",
    "    return n, len(ids)\n",
    "\n",
    "train_manifest = Path(DATASET_DIR) / \"manifest_train.jsonl\"\n",
    "val_manifest = Path(DATASET_DIR) / \"manifest_val.jsonl\"\n",
    "\n",
    "if train_manifest.exists():\n",
    "    n_train, u_train = count_manifest(train_manifest)\n",
    "    print(f\"Train: {n_train} samples from {u_train} neurons\")\n",
    "\n",
    "if val_manifest.exists():\n",
    "    n_val, u_val = count_manifest(val_manifest)\n",
    "    print(f\"Val:   {n_val} samples from {u_val} neurons\")\n",
    "    print(f\"Total: {n_train + n_val} samples from {u_train + u_val} neurons\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "\n",
    "# Show a grid: 3 neurons x 3 columns (mask_bw, mask_color, depth)\n",
    "manifest_path = Path(DATASET_DIR) / \"manifest_train.jsonl\"\n",
    "entries = []\n",
    "with open(manifest_path) as f:\n",
    "    for line in f:\n",
    "        entries.append(json.loads(line))\n",
    "\n",
    "# Pick one view from 3 different neurons\n",
    "seen_swc = set()\n",
    "selected = []\n",
    "for e in entries:\n",
    "    swc_name = e.get(\"swc\", \"\")\n",
    "    if swc_name not in seen_swc:\n",
    "        seen_swc.add(swc_name)\n",
    "        selected.append(e)\n",
    "    if len(selected) >= 3:\n",
    "        break\n",
    "\n",
    "fig, axes = plt.subplots(len(selected), 3, figsize=(12, 4 * len(selected)))\n",
    "if len(selected) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "columns = [\"mask_bw\", \"mask_color\", \"depth\"]\n",
    "titles = [\"Binary Mask (input)\", \"Segmentation (color)\", \"Depth Map\"]\n",
    "\n",
    "for row, entry in enumerate(selected):\n",
    "    for col, (key, title) in enumerate(zip(columns, titles)):\n",
    "        img_path = Path(DATASET_DIR) / entry[key]\n",
    "        img = mpimg.imread(str(img_path))\n",
    "        ax = axes[row][col]\n",
    "        cmap = \"gray\" if key != \"mask_color\" else None\n",
    "        ax.imshow(img, cmap=cmap)\n",
    "        if row == 0:\n",
    "            ax.set_title(title, fontsize=12)\n",
    "        ax.set_ylabel(Path(entry[\"swc\"]).stem[:30], fontsize=9)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train VAE \u2014 Stage 1\n",
    "\n",
    "Train the **SegVAE2D** model, a variational U-Net with:\n",
    "- A global variational bottleneck\n",
    "- Variational skip connections at each encoder level (preventing information bypass)\n",
    "- Dual-head output: semantic segmentation + depth prediction\n",
    "\n",
    "The encoder learned here becomes the image backbone for CLIP Stage 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!python -m axonet.training.trainer \\\n",
    "    --data-dir {DATASET_DIR} \\\n",
    "    --batch-size {BATCH_SIZE_VAE} \\\n",
    "    --lr {LR_VAE} \\\n",
    "    --max-epochs {MAX_EPOCHS_VAE} \\\n",
    "    --kld-weight {KLD_WEIGHT} \\\n",
    "    --skip-mode variational \\\n",
    "    --base-channels 64 \\\n",
    "    --latent-channels 128 \\\n",
    "    --num-classes 6 \\\n",
    "    --precision 32 \\\n",
    "    --save-dir {STAGE1_CKPT_DIR} \\\n",
    "    --log-dir {LOG_DIR}/stage1 \\\n",
    "    --early-stopping \\\n",
    "    --seed 42"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import glob\n",
    "\n",
    "# Find best checkpoint (prefer filename containing 'best', fall back to last)\n",
    "ckpt_files = sorted(glob.glob(f\"{STAGE1_CKPT_DIR}/*.ckpt\"))\n",
    "STAGE1_BEST = None\n",
    "for f in ckpt_files:\n",
    "    if \"best\" in f.lower():\n",
    "        STAGE1_BEST = f\n",
    "        break\n",
    "if STAGE1_BEST is None and ckpt_files:\n",
    "    STAGE1_BEST = ckpt_files[-1]  # last checkpoint\n",
    "\n",
    "print(f\"Stage 1 checkpoint: {STAGE1_BEST}\")\n",
    "print(f\"All checkpoints: {ckpt_files}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {LOG_DIR}/stage1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "from axonet.models.d3_swc_vae import load_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = load_model(Path(STAGE1_BEST), device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load a few val samples\n",
    "val_manifest = Path(DATASET_DIR) / \"manifest_val.jsonl\"\n",
    "val_entries = []\n",
    "with open(val_manifest) as f:\n",
    "    for line in f:\n",
    "        val_entries.append(json.loads(line))\n",
    "\n",
    "n_show = min(3, len(val_entries))\n",
    "fig, axes = plt.subplots(n_show, 4, figsize=(16, 4 * n_show))\n",
    "if n_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "col_titles = [\"Input (mask_bw)\", \"GT Segmentation\", \"Predicted Seg\", \"Predicted Depth\"]\n",
    "\n",
    "for row in range(n_show):\n",
    "    entry = val_entries[row]\n",
    "\n",
    "    # Load input\n",
    "    input_path = Path(DATASET_DIR) / entry[\"mask_bw\"]\n",
    "    input_img = mpimg.imread(str(input_path))\n",
    "    if input_img.ndim == 3:\n",
    "        input_img = np.mean(input_img, axis=2)\n",
    "    input_tensor = torch.from_numpy(input_img.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    # Load GT segmentation\n",
    "    gt_seg_path = Path(DATASET_DIR) / entry[\"mask\"]\n",
    "    gt_seg = mpimg.imread(str(gt_seg_path))\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        out = model(input_tensor)\n",
    "\n",
    "    pred_seg = out[\"seg_logits\"].argmax(dim=1).squeeze().cpu().numpy()\n",
    "    pred_depth = out[\"depth\"].squeeze().cpu().numpy()\n",
    "\n",
    "    # Plot\n",
    "    axes[row][0].imshow(input_img, cmap=\"gray\")\n",
    "    axes[row][1].imshow(gt_seg, cmap=\"tab10\", vmin=0, vmax=5)\n",
    "    axes[row][2].imshow(pred_seg, cmap=\"tab10\", vmin=0, vmax=5)\n",
    "    axes[row][3].imshow(pred_depth, cmap=\"magma\")\n",
    "\n",
    "    for col in range(4):\n",
    "        if row == 0:\n",
    "            axes[row][col].set_title(col_titles[col], fontsize=11)\n",
    "        axes[row][col].set_xticks([])\n",
    "        axes[row][col].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-tune CLIP \u2014 Stage 2\n",
    "\n",
    "Train a CLIP-style model that aligns the frozen VAE encoder's image embeddings with text descriptions generated from neuron metadata.\n",
    "\n",
    "- The VAE encoder is frozen; only projection heads are trained\n",
    "- Text encoder: DistilBERT (from `sentence-transformers`)\n",
    "- Loss: InfoNCE contrastive loss with learnable temperature\n",
    "- Text descriptions are auto-generated from metadata (cell type, brain region, species, morphometry)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!python -m axonet.training.clip_trainer \\\n",
    "    --stage1-checkpoint {STAGE1_BEST} \\\n",
    "    --data-dir {DATASET_DIR} \\\n",
    "    --metadata {NEURON_DIR}/metadata.jsonl \\\n",
    "    --source neuromorpho \\\n",
    "    --batch-size {BATCH_SIZE_CLIP} \\\n",
    "    --clip-embed-dim {CLIP_EMBED_DIM} \\\n",
    "    --temperature {TEMPERATURE} \\\n",
    "    --learnable-temperature \\\n",
    "    --text-encoder {TEXT_ENCODER} \\\n",
    "    --max-epochs {MAX_EPOCHS_CLIP} \\\n",
    "    --lr {LR_CLIP} \\\n",
    "    --save-dir {STAGE2_CKPT_DIR} \\\n",
    "    --log-dir {LOG_DIR}/clip \\\n",
    "    --early-stopping \\\n",
    "    --seed 42"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import glob\n",
    "\n",
    "clip_ckpts = sorted(glob.glob(f\"{STAGE2_CKPT_DIR}/*.ckpt\"))\n",
    "CLIP_BEST = None\n",
    "for f in clip_ckpts:\n",
    "    if \"best\" in f.lower():\n",
    "        CLIP_BEST = f\n",
    "        break\n",
    "if CLIP_BEST is None and clip_ckpts:\n",
    "    CLIP_BEST = clip_ckpts[-1]\n",
    "\n",
    "print(f\"CLIP checkpoint: {CLIP_BEST}\")\n",
    "print(f\"All checkpoints: {clip_ckpts}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%tensorboard --logdir {LOG_DIR}/clip"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate CLIP\n",
    "\n",
    "Run a comprehensive evaluation:\n",
    "- **Retrieval R@k**: how often the correct text/image is in the top-k results\n",
    "- **Zero-shot classification**: classify neurons by cell type and brain region using text prompts\n",
    "- **Novel query retrieval**: test with unseen text queries\n",
    "- **t-SNE visualization**: 2D embedding space colored by cell type and brain region\n",
    "\n",
    "Multi-pose images are aggregated to per-neuron embeddings via mean pooling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!python -m axonet.training.clip_evaluator \\\n",
    "    --checkpoint {CLIP_BEST} \\\n",
    "    --data-dir {DATASET_DIR} \\\n",
    "    --metadata {NEURON_DIR}/metadata.jsonl \\\n",
    "    --source neuromorpho \\\n",
    "    --output-dir {EVAL_DIR} \\\n",
    "    --pooling mean"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print the evaluation report\n",
    "from pathlib import Path\n",
    "\n",
    "report_path = Path(EVAL_DIR) / \"eval_report.txt\"\n",
    "if report_path.exists():\n",
    "    print(report_path.read_text())\n",
    "else:\n",
    "    print(\"eval_report.txt not found\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "metrics_path = Path(EVAL_DIR) / \"metrics.json\"\n",
    "if metrics_path.exists():\n",
    "    metrics = json.loads(metrics_path.read_text())\n",
    "\n",
    "    print(\"=== Retrieval Metrics ===\")\n",
    "    for k, v in metrics.get(\"retrieval\", {}).items():\n",
    "        print(f\"  {k}: {v:.1f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== Zero-Shot Classification ===\")\n",
    "    for k, v in metrics.get(\"zero_shot\", {}).items():\n",
    "        print(f\"  {k}: {v:.1f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "    print(\"\\n=== Novel Queries (top-10 precision) ===\")\n",
    "    for q, v in metrics.get(\"novel_queries\", {}).items():\n",
    "        prec = v.get(\"top_10_precision\", 0)\n",
    "        print(f\"  \\\"{q}\\\": {prec:.0f}%\")\n",
    "else:\n",
    "    print(\"metrics.json not found\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "\n",
    "tsne_cell = Path(EVAL_DIR) / \"tsne_cell_type.png\"\n",
    "tsne_region = Path(EVAL_DIR) / \"tsne_region.png\"\n",
    "\n",
    "if tsne_cell.exists():\n",
    "    print(\"t-SNE by Cell Type:\")\n",
    "    display(Image(filename=str(tsne_cell), width=700))\n",
    "\n",
    "if tsne_region.exists():\n",
    "    print(\"\\nt-SNE by Brain Region:\")\n",
    "    display(Image(filename=str(tsne_region), width=700))\n",
    "\n",
    "if not tsne_cell.exists() and not tsne_region.exists():\n",
    "    print(\"No t-SNE plots found. They may have been skipped if too few neurons.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Query\n",
    "\n",
    "Use the trained CLIP model for text-to-image retrieval: type a natural language description and retrieve the most similar neuron renderings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "from axonet.training.clip_evaluator import load_clip_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = load_clip_model(Path(CLIP_BEST), device=device)\n",
    "\n",
    "# Load val manifest and pre-compute image embeddings\n",
    "val_manifest = Path(DATASET_DIR) / \"manifest_val.jsonl\"\n",
    "val_entries = []\n",
    "with open(val_manifest) as f:\n",
    "    for line in f:\n",
    "        val_entries.append(json.loads(line))\n",
    "\n",
    "print(f\"Pre-computing embeddings for {len(val_entries)} images...\")\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for entry in val_entries:\n",
    "        input_path = Path(DATASET_DIR) / entry[\"mask_bw\"]\n",
    "        img = mpimg.imread(str(input_path))\n",
    "        if img.ndim == 3:\n",
    "            img = img.mean(axis=2)\n",
    "        tensor = torch.from_numpy(img.astype(\"float32\")).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        emb = clip_model.image_encoder.encode_for_clip(tensor)\n",
    "        all_embeddings.append(emb.cpu())\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "all_embeddings = F.normalize(all_embeddings, p=2, dim=-1)\n",
    "print(f\"Done. Embedding shape: {all_embeddings.shape}\")\n",
    "\n",
    "\n",
    "def query_neurons(text: str, top_k: int = 5):\n",
    "    \"\"\"Retrieve top-k neuron images matching a text query.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_emb = clip_model.text_encoder([text])\n",
    "        text_emb = F.normalize(text_emb.to(device), p=2, dim=-1).cpu()\n",
    "\n",
    "    sims = (all_embeddings @ text_emb.T).squeeze()\n",
    "    top_idx = torch.argsort(sims, descending=True)[:top_k].numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, top_k, figsize=(4 * top_k, 4))\n",
    "    fig.suptitle(f'Query: \"{text}\"', fontsize=13)\n",
    "    for i, idx in enumerate(top_idx):\n",
    "        entry = val_entries[idx]\n",
    "        # Show mask_color if available, else mask_bw\n",
    "        img_key = \"mask_color\" if \"mask_color\" in entry else \"mask_bw\"\n",
    "        img_path = Path(DATASET_DIR) / entry[img_key]\n",
    "        img = mpimg.imread(str(img_path))\n",
    "        ax = axes[i] if top_k > 1 else axes\n",
    "        ax.imshow(img, cmap=\"gray\" if img_key == \"mask_bw\" else None)\n",
    "        ax.set_title(f\"sim={sims[idx]:.3f}\\n{Path(entry['swc']).stem[:25]}\", fontsize=9)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run example queries\n",
    "query_neurons(\"pyramidal neuron from hippocampus\")\n",
    "query_neurons(\"interneuron from neocortex\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "| Step | What | Output |\n",
    "|------|------|--------|\n",
    "| Download | SWC morphologies from NeuroMorpho.Org | `neurons/swc/*.swc`, `metadata.jsonl` |\n",
    "| Dataset | PCA-guided multi-view rendering | `dataset/` with train/val manifests |\n",
    "| Stage 1 | SegVAE2D training (seg + depth) | Encoder checkpoint |\n",
    "| Stage 2 | CLIP fine-tuning (image-text alignment) | CLIP checkpoint |\n",
    "| Eval | Retrieval, zero-shot, t-SNE | `eval_results/` |\n",
    "\n",
    "**Scaling up for real training:**\n",
    "- Increase `N_NEURONS` to 500\u20135000+ (remove `--max-pages 1` for full query)\n",
    "- Set `MAX_EPOCHS_VAE=100`, `MAX_EPOCHS_CLIP=100`\n",
    "- Use larger batch sizes if VRAM allows\n",
    "- Consider multi-GPU with `--devices` flag"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optional: download all results as a zip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "archive_path = shutil.make_archive(\"/content/axonet_results\", \"zip\", WORK_DIR)\n",
    "print(f\"Archive created: {archive_path}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(archive_path)\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab; download manually from the file browser.\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
