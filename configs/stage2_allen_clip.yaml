# Stage 2: CLIP fine-tune on Allen Brain Institute data
experiment:
  name: "stage2_allen_clip"
  description: "CLIP contrastive fine-tuning on Allen mIVSCC-MET data"

data:
  source: "allen"
  root: "/data/allen_patchseq"
  manifest: "train.jsonl"
  metadata: "PatchSeq_metadata.csv"
  id_column: "cell_specimen_id"

rendering:
  width: 512
  height: 512
  views_per_neuron: 24
  projection: "ortho"
  cache_dir: null

model:
  base_channels: 64
  latent_channels: 128
  num_classes: 6
  skip_mode: "variational"

training:
  stage: 2
  batch_size: 32
  lr: 1.0e-4
  max_epochs: 50
  kld_weight: 0.0
  gradient_clip_val: 1.0
  precision: "32"
  num_workers: 4
  clip:
    enabled: true
    embed_dim: 512
    hidden_dim: 256
    freeze_encoder: true
    encoder_lr_mult: 0.1
    temperature: 0.07
    learnable_temperature: true
    text_encoder: "sentence-transformers/all-MiniLM-L6-v2"
    stage1_checkpoint: "checkpoints/stage1_neuromorpho/best.ckpt"
    lambda_clip: 1.0
    lambda_kld: 0.0

output:
  save_dir: "checkpoints/stage2_allen_clip"
  log_dir: "logs/stage2_allen_clip"
  save_top_k: 3
  monitor: "val_loss"
